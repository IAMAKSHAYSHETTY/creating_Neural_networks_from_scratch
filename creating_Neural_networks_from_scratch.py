# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TC7wrdixkje26RPNXFGwW9pRJxq6GWx1
"""

# Importing libraries
import numpy as np

fname = 'assign1_data.csv'

data = np.genfromtxt(fname, dtype='float', delimiter=',', skip_header=1)
X, y = data[:, :-1], data[:, -1].astype(int)
X_train, y_train = X[:400], y[:400]
X_test, y_test = X[400:], y[400:]


class DenseLayer:
    def __init__(self, n_inputs, n_neurons):
        # Initialize weights with values drawn from a normal distribution scaled by 0.01
        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        # Initialize biases to 0.0
        self.biases = 0  # np.zeros((1, n_neurons))  # This is a part I changed

    def forward(self, inputs):
        # Forward pass to compute the weighted sum and add biases
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + \
            self.biases  # This is a part I changed

    def backward(self, dz):
        # Backward pass to compute gradients of weights, biases, and inputs
        self.dweights = np.dot(self.inputs.T, dz)
        self.dbiases = np.sum(dz, axis=0, keepdims=True)
        self.dinputs = np.dot(dz, self.weights.T)


class ReLu:
    def forward(self, z):
        # Forward pass to compute the ReLU activation
        self.z = z
        self.activity = z * (z > 0)

    def backward(self, dactivity):
        # Backward pass to compute the gradient of ReLU
        self.dz = dactivity.copy()
        self.dz[self.z <= 0] = 0.0
        self.dinputs = self.dz  # Update the dinputs attribute


class Softmax:
    def forward(self, z):
        # Forward pass to compute the softmax probabilities
        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        self.probs = e_z / e_z.sum(axis=1, keepdims=True)
        return self.probs

    def backward(self, dprobs):
        # Backward pass to compute the gradient of softmax
        self.dz = np.empty_like(dprobs)
        for i, (prob, dprob) in enumerate(zip(self.probs, dprobs)):
            prob = prob.reshape(-1, 1)
            jacobian = np.diagflat(prob) - np.dot(prob, prob.T)
            self.dz[i] = np.dot(jacobian, dprob)


class CrossEntropyLoss:
    def forward(self, probs, oh_y_true):
        # Forward pass to compute the cross-entropy loss
        probs_clipped = np.clip(probs, 1e-7, 1 - 1e-7)
        loss = -np.sum(oh_y_true * np.log(probs_clipped), axis=1)
        return loss.mean(axis=0)

    def backward(self, probs, oh_y_true):
        # Backward pass to compute the gradient of cross-entropy loss
        batch_sz, n_class = probs.shape
        self.dprobs = -oh_y_true / probs
        self.dprobs = self.dprobs / batch_sz
        self.dprobs = np.nan_to_num(self.dprobs, nan=0.0)


class SGD:
    def __init__(self, learning_rate=1.0):
        # Initialize the optimizer with a learning rate
        self.learning_rate = learning_rate

    def update_params(self, layer):
        # Update weights and biases using gradient descent
        layer.weights = layer.weights - \
            (self.learning_rate * layer.dweights)  # This is a part I changed
        layer.biases = layer.biases - (self.learning_rate * layer.dbiases)

# Helper functions


def predictions(probs):
    # Get predicted labels from probabilities
    y_preds = np.argmax(probs, axis=1)
    return y_preds


def accuracy(y_preds, y_true):
    # Compute accuracy between predicted and true labels
    return np.mean(y_preds == y_true)


n_class = 3
y_true = data[:, 3].astype(int)
oh_y_true = np.eye(n_class)[y_true]


def forward_pass(X, y_true, oh_y_true):
    # Perform forward pass through the network
    dense1.forward(X)
    activation1.forward(dense1.z)
    dense2.forward(activation1.activity)
    activation2.forward(dense2.z)
    dense3.forward(activation2.activity)
    output_activation.forward(dense3.z)
    probs = output_activation.probs
    loss = crossentropy.forward(probs, oh_y_true)
    return probs, loss


def backward_pass(probs, y_true, oh_y_true):
    # Perform backward pass through the network
    crossentropy.backward(probs, oh_y_true)
    output_activation.backward(crossentropy.dprobs)
    dense3.backward(output_activation.dz)
    activation2.backward(dense3.dinputs)
    dense2.backward(activation2.dz)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dz)


np.random.seed(4)
# Set hyperparameters
epochs = 10
batch_sz = 32
learning_rate = 0.85

# Initialize network
n_inputs = X_train.shape[1]
n_classes = len(np.unique(y_train))

dense1 = DenseLayer(n_inputs, 4)
activation1 = ReLu()
dense2 = DenseLayer(4, 8)
activation2 = ReLu()
dense3 = DenseLayer(8, n_classes)
output_activation = Softmax()
crossentropy = CrossEntropyLoss()
optimizer = SGD(learning_rate)

# Training loop
for epoch in range(epochs):
    print('\n')
    print('Epoch:', epoch)
    for batch_i in range(0, X_train.shape[0], batch_sz):
        X_batch = X_train[batch_i:batch_i + batch_sz]
        y_batch = y_train[batch_i:batch_i + batch_sz]
        oh_y_batch = np.eye(n_classes)[y_batch]
        probs, loss = forward_pass(X_batch, y_batch, oh_y_batch)
        y_preds = predictions(probs)
        acc = accuracy(y_preds, y_batch)
        print('Accuracy:', acc)
        print('Loss:', loss)
        backward_pass(probs, y_batch, oh_y_batch)
        optimizer.update_params(dense1)
        optimizer.update_params(dense2)
        optimizer.update_params(dense3)

# Final Test
probs, loss = forward_pass(X_test, y_test, np.eye(n_class)[y_test])
y_preds = predictions(probs)
test_acc = accuracy(y_preds, y_test)
print('\n')
print('Test Accuracy:', test_acc)

